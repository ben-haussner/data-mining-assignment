{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for formatting each dataset\n",
    "def featureprep(train, test, dropaxis, splitset):\n",
    "\n",
    "    # Function for getting labeled dummies\n",
    "    if splitset == True:\n",
    "        def xdums(df):\n",
    "            dums = pd.get_dummies(pd.to_datetime(df[\"Date\"], format=\"%Y-%m-%d\").dt.week)\n",
    "            dums.columns = map(lambda x: \"Week_\" + str(x), dums.columns.values)\n",
    "            return dums\n",
    "    else:\n",
    "        def xdums(df):\n",
    "            dums = pd.get_dummies(df[\"Store\"])\n",
    "            dums = dums.set_index(df.index)\n",
    "            dums.columns = map(lambda x: \"Store_\" + str(x), dums.columns.values)\n",
    "            out = dums\n",
    "            dums = pd.get_dummies(df[\"Dept\"])\n",
    "            dums.columns = map(lambda x: \"Dept_\" + str(x), dums.columns.values)\n",
    "            out = out.join(dums)\n",
    "            dums = pd.get_dummies(df[\"Type\"])\n",
    "            dums.columns = map(lambda x: \"Type_\" + str(x), dums.columns.values)\n",
    "            out = out.join(dums)\n",
    "            dums = pd.get_dummies(pd.to_datetime(df[\"Date\"], format=\"%Y-%m-%d\").dt.week)\n",
    "            dums.columns = map(lambda x: \"Week_\" + str(x), dums.columns.values)\n",
    "            out = out.join(dums)\n",
    "            return out\n",
    "\n",
    "    train_x = xdums(train).join(train[[\"IsHoliday\", \"Size\", \"Year\", \"Day\", \"Days to Next Christmas\"]])\n",
    "    test_x = xdums(test).join(test[[\"IsHoliday\", \"Size\", \"Year\", \"Day\", \"Days to Next Christmas\"]])\n",
    "\n",
    "    # Deal with NAs\n",
    "    train_x = train_x.dropna(axis=dropaxis)\n",
    "    test_x = test_x.dropna(axis=dropaxis)\n",
    "    train_y = train.dropna(axis=dropaxis)[\"Weekly_Sales\"]\n",
    "\n",
    "    # Remove any train features that aren't in the test features\n",
    "    for feature in train_x.columns.values:\n",
    "        if feature not in test_x.columns.values:\n",
    "            train_x = train_x.drop(feature, axis=1)\n",
    "\n",
    "    # Remove any test features that aren't in the train features\n",
    "    for feature in test_x.columns.values:\n",
    "        if feature not in train_x.columns.values:\n",
    "            test_x = test_x.drop(feature, axis=1)\n",
    "\n",
    "    return train_x, train_y, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for returning estimates\n",
    "def estimates(train, test, splitset):\n",
    "    # Get estimates for columns that have no NAs\n",
    "    train_x, train_y, test_x = featureprep(train, test, 1, splitset)\n",
    "    estimator.fit(train_x, train_y)\n",
    "    out = pd.DataFrame(index=test_x.index)\n",
    "    out[\"Weekly_Sales\"] = estimator.predict(test_x)\n",
    "    out[\"Id\"] = out.index\n",
    "\n",
    "    # Create a dataframe for plotting the training feature regression\n",
    "    plot = pd.DataFrame(index=train_x.index)\n",
    "    plot[\"Weekly_Sales\"] = train_y\n",
    "    plot[\"Weekly_Predicts\"] = estimator.predict(train_x)\n",
    "    plot[\"Date\"] = plot.index.str.split(\"_\").str[-1]\n",
    "    plot = plot.groupby(\"Date\")[[\"Weekly_Sales\", \"Weekly_Predicts\"]].sum()\n",
    "\n",
    "    return out, plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and merging the datasets...\n"
     ]
    }
   ],
   "source": [
    "# Read in dataframes\n",
    "print(\"Reading and merging the datasets...\")\n",
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "test = pd.read_csv(\"./data/test.csv\")\n",
    "\n",
    "# Merge in store info\n",
    "stores = pd.read_csv(\"./data/stores.csv\")\n",
    "train = train.merge(stores, how='left', on='Store')\n",
    "test = test.merge(stores, how='left', on='Store')\n",
    "\n",
    "# Create indexes for submission\n",
    "train[\"Id\"] = train[\"Store\"].astype(str) + \"_\" + train[\"Dept\"].astype(str) + \"_\" + train[\"Date\"].astype(str)\n",
    "train = train.set_index(\"Id\")\n",
    "test[\"Id\"] = test[\"Store\"].astype(str) + \"_\" + test[\"Dept\"].astype(str) + \"_\" + test[\"Date\"].astype(str)\n",
    "test = test.set_index(\"Id\")\n",
    "\n",
    "# Also make an index by store_dept to split up the dataset\n",
    "train[\"Index\"] = train[\"Store\"].astype(str) + \"_\" + train[\"Dept\"].astype(str)\n",
    "test[\"Index\"] = test[\"Store\"].astype(str) + \"_\" + test[\"Dept\"].astype(str)\n",
    "\n",
    "# Add column for year\n",
    "train[\"Year\"] = pd.to_datetime(train[\"Date\"], format=\"%Y-%m-%d\").dt.year\n",
    "test[\"Year\"] = pd.to_datetime(test[\"Date\"], format=\"%Y-%m-%d\").dt.year\n",
    "\n",
    "# Add column for day\n",
    "train[\"Day\"] = pd.to_datetime(train[\"Date\"], format=\"%Y-%m-%d\").dt.day\n",
    "test[\"Day\"] = pd.to_datetime(test[\"Date\"], format=\"%Y-%m-%d\").dt.day\n",
    "\n",
    "# Add column for days to next Christmas\n",
    "train[\"Days to Next Christmas\"] = (pd.to_datetime(train[\"Year\"].astype(str)+\"-12-31\", format=\"%Y-%m-%d\") -\n",
    "                                   pd.to_datetime(train[\"Date\"], format=\"%Y-%m-%d\")).dt.days.astype(int)\n",
    "test[\"Days to Next Christmas\"] = (pd.to_datetime(test[\"Year\"].astype(str) + \"-12-31\", format=\"%Y-%m-%d\") -\n",
    "                                   pd.to_datetime(test[\"Date\"], format=\"%Y-%m-%d\")).dt.days.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the datasets into subsets...\n"
     ]
    }
   ],
   "source": [
    "# Create store_dept dictionaries\n",
    "print(\"Splitting the datasets into subsets...\")\n",
    "traindict = {}\n",
    "testdict = {}\n",
    "for index in set(test[\"Index\"].tolist()):\n",
    "    traindict[index] = train[train[\"Index\"]==index]\n",
    "    testdict[index] = test[test[\"Index\"]==index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning main model...\n",
      "Modeling... 0.5995582202587567%\n",
      "Modeling... 1.230672136320606%\n",
      "Modeling... 1.861786052382455%\n",
      "Modeling... 2.4928999684443043%\n",
      "Modeling... 3.1240138845061534%\n",
      "Modeling... 3.7551278005680024%\n",
      "Modeling... 4.3862417166298515%\n",
      "No training data available for 5_99\n",
      "Modeling... 5.017355632691701%\n",
      "Modeling... 5.64846954875355%\n",
      "Modeling... 6.2795834648154%\n",
      "Modeling... 6.910697380877248%\n",
      "Modeling... 7.541811296939098%\n",
      "Modeling... 8.172925213000946%\n",
      "Modeling... 8.804039129062796%\n",
      "Modeling... 9.435153045124645%\n",
      "Modeling... 10.066266961186495%\n",
      "Modeling... 10.697380877248342%\n",
      "Modeling... 11.328494793310192%\n",
      "Modeling... 11.959608709372041%\n",
      "Modeling... 12.590722625433893%\n",
      "Modeling... 13.22183654149574%\n",
      "No training data available for 45_39\n",
      "Modeling... 13.85295045755759%\n",
      "Modeling... 14.484064373619438%\n",
      "Modeling... 15.115178289681289%\n",
      "Modeling... 15.746292205743137%\n",
      "Modeling... 16.377406121804984%\n",
      "Modeling... 17.008520037866834%\n",
      "Modeling... 17.639633953928683%\n",
      "Modeling... 18.270747869990533%\n",
      "No training data available for 24_43\n",
      "Modeling... 18.901861786052383%\n",
      "Modeling... 19.53297570211423%\n",
      "Modeling... 20.16408961817608%\n",
      "Modeling... 20.79520353423793%\n",
      "Modeling... 21.426317450299777%\n",
      "Modeling... 22.05743136636163%\n",
      "Modeling... 22.688545282423476%\n",
      "Modeling... 23.319659198485326%\n",
      "Modeling... 23.950773114547175%\n",
      "Modeling... 24.581887030609025%\n",
      "Modeling... 25.21300094667087%\n",
      "Modeling... 25.844114862732724%\n",
      "Modeling... 26.475228778794573%\n",
      "Modeling... 27.106342694856423%\n",
      "Modeling... 27.737456610918272%\n",
      "Modeling... 28.36857052698012%\n",
      "Modeling... 28.999684443041968%\n",
      "Modeling... 29.630798359103817%\n",
      "Modeling... 30.26191227516567%\n",
      "Modeling... 30.89302619122752%\n",
      "Modeling... 31.524140107289366%\n",
      "Modeling... 32.15525402335121%\n",
      "Modeling... 32.78636793941306%\n",
      "Modeling... 33.41748185547491%\n",
      "Modeling... 34.04859577153676%\n",
      "Modeling... 34.67970968759861%\n",
      "No training data available for 9_99\n",
      "Modeling... 35.31082360366046%\n",
      "Modeling... 35.94193751972231%\n",
      "Modeling... 36.57305143578416%\n",
      "Modeling... 37.20416535184601%\n",
      "No training data available for 18_43\n",
      "Modeling... 37.83527926790786%\n",
      "Modeling... 38.46639318396971%\n",
      "Modeling... 39.09750710003156%\n",
      "Modeling... 39.728621016093406%\n",
      "Modeling... 40.359734932155256%\n",
      "No training data available for 25_99\n",
      "Modeling... 40.990848848217105%\n",
      "Modeling... 41.621962764278955%\n",
      "Modeling... 42.253076680340804%\n",
      "Modeling... 42.88419059640265%\n",
      "Modeling... 43.515304512464496%\n",
      "Modeling... 44.14641842852635%\n",
      "Modeling... 44.7775323445882%\n",
      "No training data available for 42_30\n",
      "Modeling... 45.40864626065005%\n",
      "Modeling... 46.039760176711894%\n",
      "Modeling... 46.670874092773744%\n",
      "Modeling... 47.30198800883559%\n",
      "Modeling... 47.93310192489744%\n",
      "Modeling... 48.56421584095929%\n",
      "Modeling... 49.19532975702114%\n",
      "Modeling... 49.82644367308299%\n",
      "Modeling... 50.45755758914484%\n",
      "Modeling... 51.0886715052067%\n",
      "Modeling... 51.71978542126854%\n",
      "Modeling... 52.35089933733039%\n",
      "Modeling... 52.98201325339223%\n",
      "Modeling... 53.61312716945409%\n",
      "Modeling... 54.24424108551593%\n",
      "Modeling... 54.87535500157779%\n",
      "Modeling... 55.50646891763963%\n",
      "Modeling... 56.13758283370148%\n",
      "Modeling... 56.768696749763336%\n",
      "Modeling... 57.39981066582518%\n",
      "Modeling... 58.030924581887035%\n",
      "Modeling... 58.66203849794888%\n",
      "Modeling... 59.29315241401073%\n",
      "Modeling... 59.92426633007257%\n",
      "No training data available for 10_99\n",
      "Modeling... 60.555380246134426%\n",
      "Modeling... 61.18649416219628%\n",
      "Modeling... 61.817608078258125%\n",
      "Modeling... 62.448721994319975%\n",
      "Modeling... 63.07983591038182%\n",
      "Modeling... 63.710949826443674%\n",
      "Modeling... 64.34206374250552%\n",
      "Modeling... 64.97317765856737%\n",
      "Modeling... 65.60429157462923%\n",
      "Modeling... 66.23540549069106%\n",
      "Modeling... 66.86651940675293%\n",
      "Modeling... 67.49763332281476%\n",
      "Modeling... 68.12874723887661%\n",
      "Modeling... 68.75986115493846%\n",
      "Modeling... 69.39097507100031%\n",
      "Modeling... 70.02208898706218%\n",
      "Modeling... 70.65320290312401%\n",
      "Modeling... 71.28431681918586%\n",
      "Modeling... 71.91543073524771%\n",
      "No training data available for 34_39\n",
      "Modeling... 72.54654465130956%\n",
      "Modeling... 73.17765856737141%\n",
      "Modeling... 73.80877248343326%\n",
      "Modeling... 74.43988639949511%\n",
      "Modeling... 75.07100031555696%\n",
      "Modeling... 75.70211423161881%\n",
      "Modeling... 76.33322814768066%\n",
      "Modeling... 76.9643420637425%\n",
      "Modeling... 77.59545597980436%\n",
      "Modeling... 78.2265698958662%\n",
      "No training data available for 36_30\n",
      "Modeling... 78.85768381192804%\n",
      "Modeling... 79.4887977279899%\n",
      "Modeling... 80.11991164405175%\n",
      "Modeling... 80.7510255601136%\n",
      "Modeling... 81.38213947617545%\n",
      "Modeling... 82.01325339223729%\n",
      "Modeling... 82.64436730829915%\n",
      "Modeling... 83.27548122436099%\n",
      "Modeling... 83.90659514042285%\n",
      "Modeling... 84.5377090564847%\n",
      "Modeling... 85.16882297254654%\n",
      "Modeling... 85.7999368886084%\n",
      "Modeling... 86.43105080467024%\n",
      "Modeling... 87.0621647207321%\n",
      "No training data available for 37_29\n",
      "Modeling... 87.69327863679393%\n",
      "Modeling... 88.32439255285578%\n",
      "Modeling... 88.95550646891765%\n",
      "Modeling... 89.58662038497948%\n",
      "Modeling... 90.21773430104135%\n",
      "Modeling... 90.84884821710318%\n",
      "Modeling... 91.47996213316503%\n",
      "Modeling... 92.11107604922688%\n",
      "Modeling... 92.74218996528873%\n",
      "Modeling... 93.3733038813506%\n",
      "Modeling... 94.00441779741243%\n",
      "Modeling... 94.63553171347428%\n",
      "Modeling... 95.26664562953613%\n",
      "Modeling... 95.89775954559798%\n",
      "Modeling... 96.52887346165983%\n",
      "Modeling... 97.15998737772168%\n",
      "Modeling... 97.79110129378353%\n",
      "Modeling... 98.42221520984538%\n",
      "Modeling... 99.05332912590723%\n",
      "Modeling... 99.68444304196908%\n"
     ]
    }
   ],
   "source": [
    "#Define estimator\n",
    "estimator = GradientBoostingRegressor(loss=\"huber\")\n",
    "\n",
    "\n",
    "# Run the individual store-departments models\n",
    "print(\"Beginning main model...\")\n",
    "out = pd.DataFrame()\n",
    "plot = pd.DataFrame()\n",
    "count = 0\n",
    "for key in testdict.keys():\n",
    "    count += 1\n",
    "    try:\n",
    "        ot, pt = estimates(traindict[key], testdict[key], True)\n",
    "        out = pd.concat([out, ot])\n",
    "        plot = pd.concat([plot, pt])\n",
    "    except:\n",
    "        print(\"No training data available for {}\".format(key))\n",
    "    if count%20 == 0:\n",
    "        print(\"Modeling... {}%\".format(list(testdict.keys()).index(key)/len(testdict.keys())*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>Weekly_Predicts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-02-05</th>\n",
       "      <td>2669.75</td>\n",
       "      <td>2610.505033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-12</th>\n",
       "      <td>2332.00</td>\n",
       "      <td>2329.652160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-19</th>\n",
       "      <td>3492.21</td>\n",
       "      <td>3167.497221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-26</th>\n",
       "      <td>2506.17</td>\n",
       "      <td>2357.596010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-05</th>\n",
       "      <td>2499.81</td>\n",
       "      <td>2586.238862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Weekly_Sales  Weekly_Predicts\n",
       "Date                                     \n",
       "2010-02-05       2669.75      2610.505033\n",
       "2010-02-12       2332.00      2329.652160\n",
       "2010-02-19       3492.21      3167.497221\n",
       "2010-02-26       2506.17      2357.596010\n",
       "2010-03-05       2499.81      2586.238862"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot(figsize=(20,13))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a model of all the data to fill in anything that was NA\n",
    "print(\"Creating giant model to fill in for those pesky missing datas... Probably going to take a while.\")\n",
    "sout, splot = estimates(train, test, False)\n",
    "sout = sout.join(out, how=\"left\", lsuffix=\"_Backup\")\n",
    "sout[\"Weekly_Sales\"] = sout[\"Weekly_Sales\"].fillna(sout[\"Weekly_Sales_Backup\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format for submission\n",
    "sout[\"Id\"] = sout[\"Id\"].fillna(sout[\"Id_Backup\"])\n",
    "sout = sout.drop([\"Weekly_Sales_Backup\", \"Id_Backup\"], axis=1)\n",
    "splot = splot.join(plot, how=\"left\", lsuffix=\"_Backup\")\n",
    "splot[\"Weekly_Sales\"] = splot[\"Weekly_Sales\"].fillna(splot[\"Weekly_Sales_Backup\"])\n",
    "splot[\"Weekly_Predicts\"] = splot[\"Weekly_Predicts\"].fillna(\"Weekly_Predicts_Backup\")\n",
    "splot = splot.drop([\"Weekly_Sales_Backup\", \"Weekly_Predicts_Backup\"], axis=1)\n",
    "\n",
    "sout.to_csv(\"kpalm_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the plotting file for plotting later\n",
    "sout[\"Date\"] = sout.index.str.split(\"_\").str[-1]\n",
    "plot = sout.groupby(\"Date\")[[\"Weekly_Sales\"]].sum()\n",
    "plot[\"Weekly_Predicts\"] = plot[\"Weekly_Sales\"]\n",
    "plot = plot.drop(\"Weekly_Sales\", axis=1)\n",
    "splot = splot.append(plot)\n",
    "splot = splot.reset_index().groupby(\"Date\")[[\"Weekly_Sales\", \"Weekly_Predicts\"]].sum()\n",
    "splot.to_csv(\"plot.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
