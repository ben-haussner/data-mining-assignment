{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./media/walmart_christmas.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walmart: Weihnachtsgeschäft Just in Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulprojekt für die Vorlesung __Data-Mining-Process__\n",
    "\n",
    "_Dozent:_ Prof. Dr. Johannes Maucher  \n",
    "_Studenten:_ Stephanie Flohr, Benedikt Haußner, Robert Masendorf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hinweis:** Zu korrekten Darstellung der Ergebnisse in den MarkDowns muss die Nbextension ```Python Markdown``` aktiviert sein. Zusätzlich muss ein ```Trusted```Kernel verwendet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für unser Projekt analysiseren wir Verkaufsdaten der amerikanischen Supermarktkette _Walmart_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vorgehensmodell:** CRISP-DM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://upload.wikimedia.org/wikipedia/commons/b/b9/CRISP-DM_Process_Diagram.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUSINESS UNDERSTANDING: Ursprung und grober Inhalt des Datensatzes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was ist Walmart?\n",
    "Walmart ist der wertvollste Einzelhändler der Welt mit einer Marktkapitalisierung von 246,2 Mrd. US-Dollar. Weitere Fakten:\n",
    "+ Anzahl Filialen weltweit: ca 11.600\n",
    "+ Jahresumsatz 2017: 50,3 Mrd. US-Dollar\n",
    "+ Gewinn 2017: 9,9 Mrd. US-Dollar\n",
    "+ Anzahl der Mitarbeiter weltweit: ca 2 Millionen\n",
    "\n",
    "Quelle: [Wikipedia](https://de.wikipedia.org/wiki/Walmart)\n",
    "\n",
    "### _Just in Time_ bei Walmart\n",
    "[Wikipedia](https://de.wikipedia.org/wiki/Walmart)\n",
    "\n",
    "Walmarts Strategie beruht auf ein Konzept aus geringen Preisen und gewinnmargen begleitet durch geringe Lohn- und Gehaltzahlungen trotz hohem Profit. Ein wichtiger Baustein in diesem Konzept ist auch die Produktion nach Just in Time. Walmart unterhält keine Warenlager, sondern entwickelte eine ausgefeilte Logistik, durch die die Produkte direkt aus der Produktion zu den Logistzentren geliefert werden. Dadurch sind keine Zwischenlager mehr notwendig. Um dieses Konzept weltweit umsetzen zu können, beschäftigt Walmart 2000 Analysten. \n",
    "\n",
    "### Wettbewerb auf Kaggle\n",
    "[Walmart - Nähere Informationen zur Competition](https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting).\n",
    "\n",
    "Walmart stellt den genutzen Datensatz auf Kaggle für einen Bewerberwettbewerb zur Verfügung. Mit dem Wettbewerb möchte Waltmart die besten Kandidaten für zukünftige Mitarbeiter herausfiltern. Den Bewerbern gibt diese Competition die Möglichkeit, ihr Können vorzuzeigen. \n",
    "\n",
    "Der Datensatz beinhaltet historische Verkaufdaten von 45 Walmart Märkten an unterschiedlichen Standorten. Diese sind widerum in sich in unterschiedliche Abteilungen unterteilt. Es gilt je Bereich eine Analyse der Salesdaten zu entwickeln und damit vorhersagen, welcher Bereich am effektivsten arbeitet.\n",
    "\n",
    "### Ziel unserer Arbeit\n",
    "Dieses Assignment hat nicht das Ziel, an dem Wettbewerb mitteilzunehmen. Wir haben uns als Ziel gesetzt, aus dem Daten zum Vorweihnachtsgeschäft aus 2010 das Vorweihnachtsgeschäft für 2011 vorhersagen zu können. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA UNDERSTANDING: Was wissen wir und was wollen wir wissen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der zweiten Phase des CRISP-VGMs betrachten wir die uns zur Verfügung stehenden Daten.  \n",
    "Dabei beantworten wir folgende Fragen:\n",
    "+ Was sind die Quellen unserer Daten?\n",
    "+ Wie sind die uns zur Verfügung stehenden Dateien aufgebaut?\n",
    "+ Welche Wertbereiche umfassen die Daten und welche Verteilungen sind zu erkennen?\n",
    "    + Gibt es dabei Dinge, die wir so nicht erwartet hätten?\n",
    "+ Genügt die Qualität der Daten, um in die Phase der Data Preparation übertreten zu können?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiale Datensammlung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#if not sys.warnoptions:\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das von Walmart zur Verfügung gestellte Datenpaket beinhaltet drei für uns relevante csv-Dateien:\n",
    "+ ```train.csv```\n",
    "+ ```features.csv```\n",
    "+ ```stores.csv```\n",
    "\n",
    "Die beiden Dateien ```test.csv``` und ```sampleSubmission.csv``` sind nur für den Wettbewerb interessant und werden daher hier nicht behandelt.  \n",
    "Weitere Informationen zu den einzelen Datasets: [Walmart - Data](https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beschreibung der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden werden die Attribute der einzelnen csv-Dateien kurz dargestellt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('./data/train.csv')\n",
    "display(data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Attribute ```Store```, ```Dept```und ```Date``` sind zusammen der Primärschlüssel der Tabelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ```Store```: eindeutige Filial-ID\n",
    "+ ```Dept```: pro Filiale eindeutige Abteilungs-ID\n",
    "+ ```Date```: Enddatum der Woche, für die die Verkäufe summiert wurden\n",
    "+ ```Weekly_Sales```: Wochenumsatz pro Abteilung und Filiale\n",
    "+ ```IsHoliday```: Feiertagsanzeiger\n",
    "    + ```True```, falls in der vergangenen Woche ein Feiertag lag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_df = pd.read_csv('./data/features.csv')\n",
    "display(features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Attribute ```Store``` und ```Date``` sind zusammen der Primärschlüssel der Tabelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ```Store```: eindeutige Filial-ID\n",
    "+ ```Date```: Enddatum der Woche\n",
    "+ ```Temperature```: gemittelte Temperatur der Woche\n",
    "+ ```Fuel_Price```: gemittelter Benzinpreis der Woche\n",
    "+ ```MarkDown 1-5```: Metriken, die Auskunft über spezielle Werbeangebote der Woche geben\n",
    "+ ```CPI```: Verbraucherpreisindex der Woche\n",
    "+ ```Unemployment```: Arbeitslosenquote der Woche\n",
    "+ ```IsHoliday```: Feiertagsanzeiger\n",
    "    + ```True```, falls in der vergangenen Woche ein Feiertag lag\n",
    "    + Duplikat des Attributs selbigen Namens in der Datei ```train.csv```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stores.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_df = pd.read_csv('./data/stores.csv')\n",
    "display(stores_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ```Store```: eindeutige Filial-ID\n",
    "+ ```Type```: Typ der Filiale (wobei die drei Klassen ```A```, ```B```und ```C``` vorhanden sind)\n",
    "+ ```Size```: Größe der Filiale\n",
    "    + Nachdem die Daten anonymisiert sind, sind die Angaben nicht in m<sup>2</sup> oder ft<sup>2</sup> , sondern abstrahiert zu verstehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenexploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden sollen die einzelnen Datensätze getrennt voneinander untersucht werden.  \n",
    "Von besonderem Interesse sind Wertebereiche, Extremwerte und fehlende Werte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train - Datensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Datensatz ```train.csv``` enthält die Attribute ```Store```, ```Dept```, ```Date```, ```Weekly_Sales``` und ```IsHoliday```.\n",
    "\n",
    "Im Folgenden untersuchen wir die einzelnen Attribute, wobei wir uns auf einzelne, interessante Fragestellungen beschränken, anstatt komplette univariate Analysen auszugeben.  \n",
    "\n",
    "Das Attribut ```IsHoliday``` wird hier auch von der Anzahl der Abteilungen beeinflusst.  \n",
    "Nachdem das Attribut auch im Datensatz ```features.csv``` vorkommt, wird es dort betrachtet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Store__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Datensatz umfasst __{{data_df.Store.nunique()}} einzelne Stores__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dept__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt insgesamt __{{data_df.Dept.nunique()}} verschiedene Abteilungen__.  \n",
    "Die IDs der Abteilungen reichen dabei von __{{data_df.Dept.min()}}__ bis __{{data_df.Dept.max()}}__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraglich ist, ob jeder Store auch tatsächlich jede Abteilung beherbergt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.groupby('Store')['Dept'].nunique().plot(figsize=(20, 5), kind='bar', rot=0).set_ylabel('Anzahl an Abteilungen')\n",
    "plt.axhline(data_df.Dept.nunique(), color='red')\n",
    "plt.ylim(60, data_df.Dept.nunique() + 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können beobachten, dass es nicht einen einzigen Store gibt, der alle Abteilungen unter einem Dach vereint.  \n",
    "__Der kleinste Store umfasst dabei {{data_df.groupby('Store')['Dept'].nunique().min()}} Abteilungen, der größte {{data_df.groupby('Store')['Dept'].nunique().max()}} Abteilungen__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weiterhin ist von Interesse, ob es Abteilungen gibt, die in jedem Store vorhanden sind oder eben nicht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dept_in_stores_series = data_df.groupby('Dept')['Store'].nunique()\n",
    "dept_in_stores_series.plot(figsize=(20, 5), kind='bar', rot=0).set_ylabel('Anzahl an Abteilungen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insgesamt gibt es __{{dept_in_stores_series.value_counts()[45]}} Abteilungen, die in jedem Store vertreten sind__, das sind ca {{int(dept_in_stores_series.value_counts(normalize=True)[45]*100)}} Prozent aller Abteilungen.  \n",
    "__Abteilung {{dept_in_stores_series[dept_in_stores_series==1].index.values[0]}} gibt es nur in einem einzigen Store__. Sie ist dabei die einzige Abteilung, die in nur einem Store existiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Date__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der älteste Messwert ist vom ```{{data_df.Date.min()}}```, der jüngste Messwert ist vom ```{{data_df.Date.max()}}```.  \n",
    "Insgesamt wurde an {{data_df.Date.nunique()}} Tagen gemessen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Weekly Sales__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anzahl Messpunkte:\\t{}\".format(data_df.Weekly_Sales.count()))\n",
    "print(\"Kleinster Betrag:\\t{}\".format(data_df.Weekly_Sales.min()))\n",
    "print(\"Median:\\t\\t\\t{}\".format(data_df.Weekly_Sales.median()))\n",
    "print(\"Durchschnitt:\\t\\t{0:.2f}\".format(data_df.Weekly_Sales.mean()))\n",
    "print(\"Größter Betrag:\\t\\t{}\".format(data_df.Weekly_Sales.max()))\n",
    "\n",
    "pd.DataFrame(data_df.Weekly_Sales).boxplot(figsize=(20,1), vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Interessant:__ es sind auch negative Werte für den Umsatz enthalten und das nicht zu knapp:  \n",
    "Dieses Verhalten kann in  1285 Einzelmessungen beobachtet werden, es tritt in allen Stores und Wochen auf.  \n",
    "Wie es dazu kommt wird [hier](https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/discussion/7152) erklärt, es wurden in der Periode Waren für mehr Wert retourniert als gekauft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features - Datensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Datensatz ```features.csv``` enthält die Attribute ```Store```, ```Date```, ```Temperature```, ```Fuel_Price```, ```Markdown 1-5```, ```CPI```, ```Unemployment``` und ```IsHoliday```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Store und Date__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die Datensätze ```train.csv``` und ```features.csv``` in einem späteren Schritt über die Attribute ```Store``` und ```Date``` gejoined werden sollen, wird hier überprüft, ob ein Inner Join ohne Verluste möglich ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_join_df = data_df[['Store', 'Date']]\n",
    "\n",
    "features_join_df = features_df[['Store', 'Date']]\n",
    "features_join_df = features_join_df[features_join_df.Date <= data_join_df.Date.max()]\n",
    "\n",
    "cnt_rows_without_partner = len(pd.concat([data_join_df, features_join_df]).drop_duplicates(keep=False))\n",
    "print(\"Es wurden für {} Zeilen keine Join-Partner gefunden.\".format(cnt_rows_without_partner))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Join ist also ohne Verluste machbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Temperature__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Die niedrigste gemessene Temperatur:\\t{}\".format(features_df.Temperature.min()))\n",
    "print(\"Die höchste gemessene Temperatur:\\t{}\".format(features_df.Temperature.max()))\n",
    "print(\"Median der Temperatur:\\t\\t\\t{}\".format(features_df.Temperature.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem Nachrichten über gekochte Amerikaner auch uns in Europa erreicht hätten, gehen wir davon aus, dass die Temperatur in Fahrenheit vorliegt.  \n",
    "In einem ersten Schritt wollen wir diese umrechnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fahr_to_celsius(temp_fahr):\n",
    "    return round((temp_fahr - 32) * 5 / 9, 2)\n",
    "\n",
    "features_df['Temperature_F'] = features_df.Temperature\n",
    "features_df['Temperature_C'] = features_df.Temperature.apply(lambda tmp_fahr: fahr_to_celsius(tmp_fahr))\n",
    "features_df.drop(columns=['Temperature'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Die niedrigste gemessene Temperatur:\\t{}\".format(features_df.Temperature_C.min()))\n",
    "print(\"Die höchste gemessene Temperatur:\\t{}\".format(features_df.Temperature_C.max()))\n",
    "print(\"Median der Temperatur:\\t\\t\\t{}\".format(features_df.Temperature_C.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Ergebnisse ergeben durchaus mehr Sinn aus der europäischen Perspektive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_stores = features_df[features_df.Temperature_C == features_df.Temperature_C.min()].Store.tolist() + features_df[features_df.Temperature_C == features_df.Temperature_C.max()].Store.tolist()\n",
    "temperature_store_df = pd.DataFrame()\n",
    "\n",
    "for store in list_of_stores:\n",
    "    temperature_store_df[store] = features_df.Temperature_C[features_df.Store == store].tolist()\n",
    "    temperature_store_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "temperature_store_df.boxplot(figsize=(20,5), vert=False)\n",
    "plt.xlabel(\"Temperatur\")\n",
    "plt.ylabel(\"Store\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fuel_Price__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Der niedrigste Preis pro Gallone Sprit:\\t\\t{}\".format(features_df.Fuel_Price.min()))\n",
    "print(\"Der höchste Preis pro Gallone Sprit:\\t\\t{}\".format(features_df.Fuel_Price.max()))\n",
    "print(\"Median des Spritpreises pro Gallone:\\t\\t{}\".format(features_df.Fuel_Price.median()))\n",
    "\n",
    "pd.DataFrame(features_df.Fuel_Price).boxplot(figsize=(20,1), vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__MarkDown 1-5__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Beschreibung auf [Kaggle](https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data) entnehmen wir, dass die Werte für die Marketingaktionen erst ab November 2010 verfügbar sind. Zudem fehlen angeblich Daten für weitere Wochen.  \n",
    "Nachdem wir die Werte ab September 2010 für die Prognose nutzen wollen, könnte es sein, dass die Attribute für uns wertlos sind.\n",
    "\n",
    "Auf jeden Fall soll an dieser Stelle untersucht werden, wie es sich mit fehlenden Werten im Datensatz generell verhält:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(features_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wollen wissen, für welchen Zeitraum tatsächlich Daten der Attribute ```MarkDown 1-5``` vorhanden sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Erstes Datum mit allen Werten für MarkDown:\\t{}\".format(features_df[['Date', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']].dropna().Date.min()))\n",
    "print(\"Letztes Datum mit allen Werten für MarkDown:\\t{}\".format(features_df[['Date', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']].dropna().Date.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem wir aus den Daten von 2010 für das Weihnachtsgeschäft von 2011 lernen wollen, sind die Attribute ```MarkDown 1-5``` für uns wertlos und werden in der Phase der __Data Preparation__ entfernt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CPI__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Verbrauchspreisindex zeigt die Steigerung (oder Minderung) der Lebenshaltungskosten an.  \n",
    "Dabei existieren verschiedene CPIs - mit welchem genau wir es zu tun haben und welches das Basisjahr ist, konnten wir nicht ermitteln.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Der niedrigste Wert des CPI:\\t{}\".format(features_df.CPI.min()))\n",
    "print(\"Der höchste Wert des CPI:\\t{}\".format(features_df.CPI.max()))\n",
    "print(\"Median des CPI:\\t\\t\\t{}\".format(features_df.CPI.median()))\n",
    "\n",
    "pd.DataFrame(features_df.CPI).boxplot(figsize=(20,1), vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie soeben in der Auswertung der ```MarkDown```-Attribute gesehen, fehlen auch hier Werte.  \n",
    "Diese fehlenden Werte müssen, falls nötig, in der Phase der __Data Preparation__ nachberechnet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi_df = features_df.set_index(keys=['Store', 'Date'])['CPI'].unstack(level='Store')\n",
    "cpi_df.index = pd.to_datetime(cpi_df.index)\n",
    "cpi_df.plot(figsize=(20,13))\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('CPI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dieses Finding ist interessant:__  \n",
    "Warum gibt es Regionen, in denen der CPI den minimalen Wert von ```{{features_df.CPI.min()}}``` hat, im Gegensatz zu Regionen, bei denen der maximale Wert ```{{features_df.CPI.max()}}```, also fast das Doppelte, beträgt?  \n",
    "Zumindest in Deutschland ließe sich ein derartiger Unterschied im Einzelhandel nicht regional beurteilen.  \n",
    "Wir werden dieses Thema später eventuell nochmals aufgreifen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Unemployment__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Der niedrigste Wert des Unemployment:\\t{}\".format(features_df.Unemployment.min()))\n",
    "print(\"Der höchste Wert des Unemployment:\\t{}\".format(features_df.Unemployment.max()))\n",
    "print(\"Median des Unemployment:\\t\\t{}\".format(features_df.Unemployment.median()))\n",
    "\n",
    "mean_umempl_usa_201011 = (8.9 + 8.1) / 2\n",
    "\n",
    "pd.DataFrame(features_df.Unemployment).boxplot(figsize=(20,1), vert=False)\n",
    "plt.axvline(x=mean_umempl_usa_201011, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben die Daten des Datensatzes mit der durchschnittlichen Arbeitslosenquote der Vereinigten Staaten aus den Jahren 2011 und 2012 (Quelle: [Wikipedia](https://de.wikipedia.org/wiki/Arbeitsmarktstatistik_der_Vereinigten_Staaten#2011_bis_2018)) in Höhe von ```{{mean_umempl_usa_201011}}``` verglichen.\n",
    "\n",
    "Dabei zeigt sich, dass die Daten in Walmarts erhoben wurden, die potentiell eher in Regionen mit niedrigerer Arbeitslosenquote angesiedelt sind.  \n",
    "Dieses zeigt sich auch in den Ausreißern: während in Q1 und Q2 die Daten eher dicht gepackt sind, streut Q3 und vor allem Q4 stark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__IsHoliday__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(features_df.IsHoliday).apply(pd.value_counts).plot.pie(figsize=(5,5), y='IsHoliday', autopct='%.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie in der Grafik zu sehen, sind in ```7.14%``` der betrachteten Datensätze Feiertage zu verzeichnen.  \n",
    "In diesem Zeitraum liegen unter anderem:\n",
    "+ Super Bowl\n",
    "+ Labor Day\n",
    "+ Thanksgiving\n",
    "+ Black Friday\n",
    "+ Weihnachten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stores - Datensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Datensatz ```stores.csv``` enthält die Attribute ```Store```, ```Type``` und ```Size```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Store__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Store``` stellt für die Datei den Primärschlüssel, dabei sind alle Stores von ID ```{{stores_df.Store.min()}}``` bis ID ```{{stores_df.Store.max()}}``` enthalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Type__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Attribut ```Type``` enthält die Werte ```{{stores_df.Type.unique()[0]}}```, ```{{stores_df.Type.unique()[1]}}``` und ```{{stores_df.Type.unique()[2]}}```.  \n",
    "Es gibt folgende unterschiedliche Typen von Märkten (Quelle: [Walmart Store Locator](https://corporate.walmart.com/our-story/our-business)), wobei wir uns nicht sicher sind, inwieweit dies gemapped werden kann:\n",
    "+ Walmart Supercenter\n",
    "+ Walmart Discount Store\n",
    "+ Walmart Neighbourhood Market\n",
    "\n",
    "Die Verteilung der Store-Typen gestaltet sich wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_df.groupby('Type')['Type'].count().plot(kind='bar', rot=0)\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Size__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die Daten anonymisiert sind, kann keine definitive Aussage darüber getroffen werden, in welcher Einheit die gegebenen Werte im Bereich von ```{{stores_df.Size.min()}}``` bis ```{{stores_df.Size.max()}}``` vorliegen.  \n",
    "Allerdings entsprechen die Durchschnittswerte für die einzelnen Typen ziemlich exakt den Typen von Walmarts, die im letzten Punkt angesprochen worden waren:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/walmart_types.jpg\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_a_type = 182000\n",
    "size_b_type = 106000\n",
    "size_c_type = 38000\n",
    "\n",
    "stores_df.boxplot(column='Size', by='Type', vert=False, figsize=(20,5))\n",
    "plt.axvline(x=size_a_type, color='red')\n",
    "plt.axvline(x=size_b_type, color='red')\n",
    "plt.axvline(x=size_c_type, color='red')\n",
    "\n",
    "plt.suptitle('')\n",
    "plt.title('')\n",
    "plt.xlabel('Size')\n",
    "plt.ylabel('Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erste Hypothesen\n",
    "Um erste Hypothesen aufstellen zu können, die über unsere bisherigen Betrachtungen (beispielsweise zur Arbeitslosenquote) hinausgehen, müssen wir die Datensätze miteinander verbinden. Hierfür verweisen wir auf die Phase der __Data Preparation__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Betrachtung der Datenqualität\n",
    "Die mangelnde Qualität der Features ```MarkDown 1-5``` wurde bereits angesprochen.  \n",
    "Darüber hinausgehende Aussagen können erst in der Phase der __Data Preparation__ getroffen werden.\n",
    "\n",
    "Auf den ersten Blick sieht es so aus, als würden die zur Verfügung stehenden Daten ausreichen, um erste Modelle zu bauen. Sie enthalten vorerst alle wichtigen Informationen. Falls die Qualität der Modelle nicht zufriedenstellend ist, ist die Struktur geeignet, um durch weitere Daten erweitert zu werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARATION: Neural Net Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden bereiten wir die soeben untersuchten Daten für das Modellieren vor.  \n",
    "Dabei wird zuerst ein __zeitlicher Zuschnitt__ erfolgen, an den sich ein __Feature Engineering__ anschließt.  \n",
    "Nach einer __Normalisierung__ sollen die Daten in 3D-Numpy-Arrays aufgebaut werden, um ein Training mit __Moving Window__ in __neuronalen Netzen__ zu ermöglichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeitlicher Zuschnitt der Daten auf das Projektziel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie bereits erwähnt, ist es unser Ziel, aus den Daten von 2010 und 2011 das Weihnachtsgeschäft des Jahres 2011 zu prognostizieren. Daher fokussieren wir uns auf diesen Zeitraum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.Date = pd.to_datetime(data_df.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xmas_data_df = data_df[((data_df.Date >= '2010-02-05') & (data_df.Date <= '2011-12-31'))]\n",
    "xmas_data_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(xmas_data_df.loc[(xmas_data_df['Store'] == 1) & (xmas_data_df['Dept'] ==1)].head(2))\n",
    "display(xmas_data_df.loc[(xmas_data_df['Store'] == 1) & (xmas_data_df['Dept'] ==1)].tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir betrachten also einen Zeitraum von ```{{xmas_data_df.Date.nunique()}}``` Wochen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraglich ist, ob die Daten von zwei Jahren hier schon ausreichend einem Muster folgen - speziell, weil wir direkt die Ergebnisse des zweiten Weihnachtsgeschäfts prognostizieren wollen.  \n",
    "Dazu visualisieren wir die beiden Jahre und betrachten vor allem die Wochen vor Weihnachten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas_data_df.groupby('Date').agg(['sum']).reset_index().plot(x='Date', y='Weekly_Sales',kind=\"bar\", figsize=(20,7)).set_ylabel(\"Weekly Sales in 10 mio.\")\n",
    "plt.axhline(y=xmas_data_df['Weekly_Sales'][xmas_data_df.Date == '2010-11-26'].agg(['sum'])[0], color='black')\n",
    "plt.axhline(y=xmas_data_df['Weekly_Sales'][xmas_data_df.Date == '2010-12-03'].agg(['sum'])[0], color='grey')\n",
    "plt.axhline(y=xmas_data_df['Weekly_Sales'][xmas_data_df.Date == '2010-12-10'].agg(['sum'])[0], color='green')\n",
    "plt.axhline(y=xmas_data_df['Weekly_Sales'][xmas_data_df.Date == '2010-12-17'].agg(['sum'])[0], color='orange')\n",
    "plt.axhline(y=xmas_data_df['Weekly_Sales'][xmas_data_df.Date == '2010-12-24'].agg(['sum'])[0], color='red')\n",
    "# Anpassung y-Achse über min- und max-Werte mit Faktor 0.1\n",
    "plt.ylim(xmas_data_df.groupby('Date').agg(['sum']).Weekly_Sales.min()[0] * 0.9, xmas_data_df.groupby('Date').agg(['sum']).Weekly_Sales.max()[0] * 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dieser Grafik lässt sich ein Muster des Verlaufs der Verkaufszahlen in der Weihnachstzeit erkennen, weshalb wir erhoffen, gute Prognosen aus den Daten ableiten zu können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warnung: nächste Woche ist ein wichtiger Feiertag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgrund der Bedeutung der Feiertage für die Verkaufszahlen, erweitern wir den Datensatz um da Feature ```next_week```, das folgende Werte annehmen kann:\n",
    "+ ```black_friday```\n",
    "+ ```xmas_1st_week```\n",
    "+ ```xmas_2nd_week```\n",
    "+ ```xmas_3rd_week```\n",
    "+ ```xmas_4th_week```\n",
    "+ ```new_year```\n",
    "\n",
    "Die Hoffnung ist, dass die Algorithmen aus diesem Hinweis für die nächste Woche sinnvolle Schlüsse ziehen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas_data_df.loc[xmas_data_df.Date == '2010-11-19', 'next_week'] = 'black_friday'\n",
    "xmas_data_df.loc[xmas_data_df.Date == '2010-11-26', 'next_week'] = 'xmas_1st_week'\n",
    "xmas_data_df.loc[xmas_data_df.Date == '2010-12-03', 'next_week'] = 'xmas_2nd_week'\n",
    "xmas_data_df.loc[xmas_data_df.Date == '2010-12-10', 'next_week'] = 'xmas_3rd_week'\n",
    "xmas_data_df.loc[xmas_data_df.Date == '2010-12-17', 'next_week'] = 'xmas_4th_week'\n",
    "xmas_data_df.loc[xmas_data_df.Date == '2010-12-24', 'next_week'] = 'new_year'\n",
    "\n",
    "xmas_data_df.loc[xmas_data_df.Date == '2011-11-18', 'next_week'] = 'black_friday'\n",
    "xmas_data_df.loc[xmas_data_df.Date == '2011-11-25', 'next_week'] = 'xmas_1st_week'\n",
    "xmas_data_df.loc[xmas_data_df.Date == '2011-12-02', 'next_week'] = 'xmas_2nd_week'\n",
    "xmas_data_df.loc[xmas_data_df.Date == '2011-12-09', 'next_week'] = 'xmas_3rd_week'\n",
    "xmas_data_df.loc[xmas_data_df.Date == '2011-12-16', 'next_week'] = 'xmas_4th_week'\n",
    "xmas_data_df.loc[xmas_data_df.Date == '2011-12-23', 'next_week'] = 'new_year'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wie lange dauerts noch bis Weihnachten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zusätzlich wäre es interessant zu erfahren, wie viele Tage es noch bis Weihnachten sind.  \n",
    "Dazu dient das Feature ```days_2_xmas```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xmas_data_df['days_2_xmas'] = xmas_data_df.Date.apply(lambda x: (date(x.year,12,25) - x.date()).days)\n",
    "xmas_data_df[['Date', 'days_2_xmas']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Umgang mit kategorialen Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sowohl das soeben erstellte Attribut ```next_week```, als auch die Attribute ```Store_Type``` und ```IsHoliday``` sollten betrachtet werden, da die von uns eingesetzten neuronalen Netze nicht mit kategorialen Daten umgehen können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### next_week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier können wir praktischerweise die von ```pandas``` mitgelieferte Funktion ```get_dummies``` nutzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xmas_data_df = pd.get_dummies(data=xmas_data_df, columns=['next_week'], prefix='nw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas_data_df[(xmas_data_df.Store == 1) & (xmas_data_df.Dept == 1) & (xmas_data_df.Date >= '2010-11-12') & (xmas_data_df.Date < '2010-12-31')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Type' in stores_df.columns:\n",
    "    stores_df = pd.get_dummies(data=stores_df, columns=['Type'], prefix='type')\n",
    "stores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IsHoliday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine gesonderte Behandlung für ```IsHoliday``` nicht eventuell nötig, weil ```boolean```sich in ```int``` casten lässt.  \n",
    "Wir gehen allerdings auf Nummer sicher und nehmen den Netzen diese Arbeit ab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.IsHoliday = features_df.IsHoliday.astype(int)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da das Feature ```IsHoliday``` neben dem soeben umgewandelten ```IsHoliday``` noch redundant im File ```train.csv``` vorhanden ist, wird dieses hier gedroppt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.Date = pd.to_datetime(features_df.Date)\n",
    "if 'IsHoliday' in xmas_data_df.columns:\n",
    "    xmas_data_df.drop(columns=['IsHoliday'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem wir die Daten so erstellt haben, wie wir sie für den ersten Versuch nutzen wollen, können die Datensets ```features_df```, ```stores_df``` und ```xmas_data_df``` gemerged werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(xmas_data_df,features_df,on=['Store','Date'])\n",
    "merged_all_df = pd.merge(merged_df,stores_df,on='Store')\n",
    "merged_all_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie man in der Ausgabe oben sehen kann, enthält der Datenbestand noch __```NaN```-Werte__. Nachdem ein geeigneter Umgang mit diesen gefunden wurde, sollen noch die __Korrelationen__ betrachtet werden, um eventuell vorhandene und quasi redundante Features zu entfernen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Umgang mit NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuerst können direkt wie bereits erwähnt die Attribute ```MarkDown 1-5 ``` entfernt werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'MarkDown1' in merged_all_df.columns:\n",
    "    merged_all_df.drop(columns=['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'],inplace=True)\n",
    "merged_all_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend wird noch geklärt, inwieweit weitere ```NaN```-Werte vorhanden sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier scheinen wir Glück gehabt zu haben, weitere Aktionen wie Mitteln ect von Werten sind nicht nötig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Korrelationen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden plotten wir uns die Korrelationsmatrix für unsere Features.  \n",
    "Der Code ist der [Seaborn Doku](https://seaborn.pydata.org/examples/many_pairwise_correlations.html) entnommen und leicht angepasst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = merged_all_df.iloc[:,3:].corr()\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax = plt.subplots(figsize=(20, 9))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, center=0, square=False, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man kann direkt erkennen, dass es eine hohe Korrelation der Features ```Temperature_F``` und ```Temperature_C``` gibt - was verständlich ist, da wir ```Temperature_F``` selbst umgerechnet haben. Wir löschen ```Temperature_F```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Temperature_F' in merged_all_df.columns:\n",
    "    merged_all_df.drop(columns='Temperature_F',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es freut uns natürlich, dass es eine Korrelation zwischen ```nw_new_year``` und ```Weekly_Sales``` gibt - diese Feature könnte also genutzt werden, um dieses einmalige Ereignis wiederzuerkennen.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwischen ```nw_xmas_1st_week``` und ```IsHoliday``` besteht ein Zusammenhang, da in dieser Woche immer der _Black Friday_ liegt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spannend ist auch, dass es anscheinend einen Zusammenhang zwischen der ```Temperatur_C``` und ```Fuel_Price```, ```Unemployment``` und ```CPI``` gibt - während ```CPI```und ```Fuel_Price``` tatsächlich eine negative Korrelation aufweisen und ```Unemployment``` und ```Fuel_Price``` neutral zueinander stehen.  \n",
    "Dass ```CPI``` und ```Unemployment``` wiederum negativ korrelieren ergibt Sinn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch, dass es anscheinend an den interessanten Feiertagen eher kälter ist, überrascht nicht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch die Zusammenhänge zwischen (Store-)```Size```, (Store-)```Type``` und absoluten ```Weekly_Sales``` überrascht nicht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fazit:__\n",
    "Wir sind am Ende der Datenbereinigung angekommen und haben damit einen Datensatz mit ```{{merged_all_df.shape[0]}}``` Messpunkten und aktuell ```{{merged_all_df.shape[1]}}``` Attibuten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An dieser Stelle würde im Normalfall eine Normalisierung der Daten anstehen, da die Skalen wie bereits gesehen sehr unterschiedlich ausfallen, nicht nur zwischen den ehemals kategorialen und den stetigen Attributen.  \n",
    "Um zu ermöglichen, dass die Daten in Trainings- und Testdaten gesplittet werden können und die Normalisierung nur anhand der Trainingsdaten stattfinden kann, findet die Normalisierung in der Splitting-Funktion statt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary bauen: Neural Network Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die einzelnen im Folgenden erzeugten Dataframes, Arrays, Modelle ect sinnvoll im Speicher vorhalten zu können, arbeiten wir mit einem Dictionary, das in drei Ebenen angelegt ist:  \n",
    "__dict [ Store-Department-Identifier ] [ einzelne Dateframes, Arrays, ect und Modeltyp [ Modell, Auswertung ect ] ]__\n",
    "\n",
    "Die Idee haben wir von [Kevin Palmer](https://github.com/kevinpalm/walmart_sales_forecast/blob/master/final_model.py) geklaut, die Umsetzung erfolgte allerdings auf sehr unterschiedliche Weise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die weiteren Arbeiten sollen immer auf Basis einer Store-Department-Kombination erfolgen (zb für ```Store 1``` und ```Department 1```).  \n",
    "Dies bedeutet, dass pro solcher Kombination Modelle gebaut und bewertet werden sollen.  \n",
    "Dazu generieren wir Identifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df['Store_Dept'] = merged_all_df.Store.astype(str) + '_' + merged_all_df.Dept.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben es insgesamt mit ```{{merged_all_df.Store_Dept.nunique()}}``` Kombinationen zu tun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden werden diese Identifier genutzt, um die erste Ebene des Dictionaries anzulegen.  \n",
    "Dabei ist zu beachten, dass der folgende Code nicht standardmäßig ausgeführt wird, um die weitere Ausführung des Notebooks nicht unverhältnismäßig zu verlangsamen.  \n",
    "Im Bedarfsfall sollte die nächste Zelle aktiviert und die übernächste Zelle (\"Abgespeckte Version\") deaktiviert werden:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for store_dept in set(merged_all_df.Store_Dept.tolist()):\n",
    "    multi_dict[store_dept] = {}\n",
    "print(len(multi_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit die Prüfungen im weitern Verlauf alle funktionieren, __MUSS__ die Store-Department-Kombination ```1_1``` __IMMER__ enthalten sein!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_dept_subgroup = ['1_1', '2_1']\n",
    "\n",
    "for store_dept in store_dept_subgroup:\n",
    "    multi_dict[store_dept] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Unterkapitel nehmen wir die Aufspaltung nach __Trainings-, Test- und Validationsdaten__ vor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation je Store und Depatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/calendar.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ziel ist es, aus den Daten der schwarz markierten Wochen die Verkaufszahlen der grün markierten Woche zu ermitteln. Die Daten sind dabei jeweils am Freitag für die vergangene Woche erfasst.  \n",
    "Die rot markierte Woche ist überflüssig und kann gelöscht werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cutoff = date(2011,12,30)\n",
    "merged_all_df.drop(merged_all_df[(merged_all_df.Date >= date_cutoff)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit ist das jüngste Datum im Datenbestand der ```{{merged_all_df.Date.max()}}```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Anmerkung:__  \n",
    "Da Bäume einen anderen Aufbau der Daten benötigen, kopieren wir den aktuellen Stand des Dataframes, da wir weitere Änderungen direkt auf dem Dataframe machen werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_for_tree_df = merged_all_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können die oben markierten Validierungsdaten für alle Store-Department-Kombinationen ermittelt und in das Dictionary gespeichert werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date_start_windowing = date(2011,11,25)\n",
    "date_validation_week = date(2011,12,23)\n",
    "\n",
    "if date_validation_week in merged_all_df.Date.dt.date.values:\n",
    "    for store_dept in multi_dict.keys():\n",
    "        multi_dict[store_dept]['val_data_df'] = merged_all_df[(merged_all_df.Date >= date_start_windowing)&(merged_all_df.Store_Dept == store_dept)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit hat das Dictionary folgenden Inhalt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict['1_1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict['1_1']['val_data_df'][['Store', 'Dept', 'Date', 'Weekly_Sales']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um ein Leaken der Validierungsdaten ins Trainig zu verhindern, wird die entscheidende Woche aus dem Dataframe getilgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df.drop(merged_all_df[(merged_all_df.Date == date_validation_week)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das jüngste Datum ist damit der ```{{merged_all_df.Date.max()}}``` und die Validierungs-Woche ist nicht mehr enthalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all_df[(merged_all_df.Date >= date_start_windowing)&(merged_all_df.Store_Dept == '1_1')][['Store', 'Dept', 'Date', 'Weekly_Sales']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test-Daten je Store und Department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die Validierungsdaten sauber abgespeichert sind, kümmern wir uns um die Trainings- und Testdaten.  \n",
    "Dabei nehmen wir auch eine Restrukturierung vor, wobei vor allem das Attribut ```Weekly_Sales``` an den Schluss geschoben wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure_df_for_target_variable(df):\n",
    "    return df[['Date',\n",
    "       'nw_black_friday', 'nw_new_year', 'nw_xmas_1st_week', 'nw_xmas_2nd_week',\n",
    "       'nw_xmas_3rd_week', 'nw_xmas_4th_week', 'days_2_xmas', 'Fuel_Price',\n",
    "       'CPI', 'Unemployment', 'IsHoliday', 'Temperature_C', 'Size', 'type_A',\n",
    "       'type_B', 'type_C', 'Weekly_Sales']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_dept in multi_dict.keys():\n",
    "    multi_dict[store_dept]['train_test_df'] = restructure_df_for_target_variable(merged_all_df[(merged_all_df.Store_Dept == store_dept)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit hat das Dictionary die folgende Form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict['1_1'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und der Dataframe sieht wie folgt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict['1_1']['train_test_df'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test-Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die Trainigs- und Testdaten isoliert wurden, kann jetzt der Split erfolgen.  \n",
    "Im Rahmen des Splittes findet wie gesagt auch die __Normalisierung__ statt.  \n",
    "Am besten man beginnt beim Studium folgender Funktionen mit der Funktion ```split_data()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data_np, scaler=None):\n",
    "    if not scaler:\n",
    "        scaler = StandardScaler().fit(data_np)\n",
    "    return scaler.transform(data_np), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_scalers(data_df):\n",
    "    data_np = data_df.drop(columns='Date').values\n",
    "    X_scaler = StandardScaler().fit(data_np[:,:-1])\n",
    "    y_scaler = StandardScaler().fit(data_np[:,-1].reshape(-1, 1))\n",
    "    return X_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_y_split(data_df, lookback, delay, X_y_scaler=None):\n",
    "    data_np = data_df.drop(columns='Date').values\n",
    "    data_rows = np.arange(lookback, data_np.shape[0] - delay)\n",
    "    \n",
    "    scaled_data_np, X_y_scaler = scale_data(data_np, X_y_scaler)\n",
    "\n",
    "    X_np = np.zeros((len(data_rows), lookback, scaled_data_np.shape[1]-1))\n",
    "    y_np = np.zeros((len(data_rows),))\n",
    "    \n",
    "    for i, row in enumerate(data_rows):\n",
    "        indices = range(row - lookback, row)\n",
    "        X_np[i] = scaled_data_np[indices,:-1]\n",
    "        y_np[i] = scaled_data_np[row + delay][-1]\n",
    "    \n",
    "    return X_np, y_np, X_y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, lookback, delay, cutoff_date):\n",
    "    train_samples, train_targets, X_y_scaler = X_y_split(data[data.Date <= cutoff_date], lookback, delay)\n",
    "    test_samples, test_targets, X_y_scaler = X_y_split(data[data.Date > cutoff_date], lookback, delay, X_y_scaler)\n",
    "    return train_samples, train_targets, test_samples, test_targets, X_y_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die Funktionenn definiert wurden, werden die Parameter für den Split gesetzt.  \n",
    "Wir wollen vier Wochen aus der Vergangenheit betrachten, um die aktuelle Woche zu prognostizieren, die Daten ab dem ```2011-05-01``` nutzen wir zum Testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 4\n",
    "delay = 0\n",
    "cutoff_date = '2011-05-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im ersten Schritt speichern wir die beiden Scaler für ```X_scaler``` und ```y_scaler```, die wir später für den Sanity Check benötigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_dept in multi_dict.keys():\n",
    "    df = multi_dict[store_dept]['train_test_df']\n",
    "    X_scaler, y_scaler = get_X_y_scalers(df[df.Date <= cutoff_date])\n",
    "    multi_dict[store_dept]['X_scaler'] = X_scaler\n",
    "    multi_dict[store_dept]['y_scaler'] = y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict['1_1'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im zweiten Schritt nehmen wir den Train-Test-X-y-Split vor.  \n",
    "Dabei speichern wir zusätzlich den ```X-y-Scaler``` ab, den wir später für den Validierungsdatensatz benötigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_dept in multi_dict.keys():\n",
    "    X_train, y_train, X_test, y_test, X_y_scaler = split_data(multi_dict[store_dept]['train_test_df'], lookback, delay, cutoff_date)\n",
    "    multi_dict[store_dept]['X_train'] = X_train\n",
    "    multi_dict[store_dept]['y_train'] = y_train\n",
    "    multi_dict[store_dept]['X_test'] = X_test\n",
    "    multi_dict[store_dept]['y_test'] = y_test\n",
    "    multi_dict[store_dept]['X_y_scaler'] = X_y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict['1_1'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sanity Check:__\n",
    "Nach den nicht ganz unkomplexen Vorgängen wollen wir wissen, ob das Ergebnis unseren Erwartungen entspricht:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der ```X_train```-Datensatz sollte ein 3D-Numpy-Array sein, das ```61 Iterationen``` von ```4-Wochen-Vorschauen``` mit ```je 16 Attributen``` enthält:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict['1_1']['X_train'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das passt.  \n",
    "Jetzt lesen wir die erste Woche der Iteration aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict['1_1']['X_train'][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem uns die normalisierten Werte nicht viel sagen, rechnen wir diese mit dem ```X_scaler``` zurück:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict['1_1']['X_scaler'].inverse_transform(multi_dict['1_1']['X_train'][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und vergleichen diese mit der ersten Woche im unbehandelten Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict['1_1']['train_test_df'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Fazit:__ Das sieht soweit ganz gut aus!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt bleibt noch, ```y_train``` für die erste Woche auf die selbe Art zu prüfen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict['1_1']['y_train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict['1_1']['y_scaler'].inverse_transform(multi_dict['1_1']['y_train'][0].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict['1_1']['train_test_df'].Weekly_Sales.iloc[lookback + delay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict['1_1']['train_test_df'].iloc[4].Weekly_Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Fazit:__ Auch das passt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation to np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch die Validations-Daten sollten als np-Array vorliegen, um später Auswertungen machen zu können:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_dept in multi_dict.keys():\n",
    "    multi_dict[store_dept]['val_data_df'] = restructure_df_for_target_variable(multi_dict[store_dept]['val_data_df'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_dept in multi_dict.keys():\n",
    "    X_np, y_np, foo = X_y_split(multi_dict[store_dept]['val_data_df'], lookback, delay, multi_dict[store_dept]['X_y_scaler'])\n",
    "    multi_dict[store_dept]['X_val'] = X_np\n",
    "    multi_dict[store_dept]['y_val'] = y_np\n",
    "multi_dict['1_1'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set prediction goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zuverlässig auf das Ziel der Validations-Daten zugreifen zu können, wir auch dieses abgespeichert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_dept in multi_dict.keys():\n",
    "    multi_dict[store_dept]['prediction_goal'] = multi_dict[store_dept]['val_data_df'].iloc[-1].Weekly_Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict['1_1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dict['1_1']['prediction_goal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELING: Neural Network Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach den raumgreifenden Erläuterungen zur Data Preparation erscheint das folgende Kapitel fast karg.  \n",
    "Zuerst betrachten wir eine einfache __Baseline__.  \n",
    "Dann __trainieren__ wir verschiedene Algorithmen mit den erstellten Daten.  \n",
    "Im Anschluss wagen wir eine __kurze Evaluation__ der ersten Ergebnisse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Baseline folgt dem einfachen Prinzip, dass die ```Weekly_Sales``` der aktuellen Woche als Prognose für die kommende Woche angenommen werden. Dies mag unter dem Jahr eine gar nicht so schlechte Methode sein (im Vergleich zum Input), für das Weihnachtsgeschäft kann sie allerdings nur versagen.\n",
    "Der Einfachheit halber erfolgt die Erstellung nur für Store-Department-Kombination ```1_1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_prediction_values(df):\n",
    "    prognose_sales = df.iloc[-2].Weekly_Sales\n",
    "    actual_sales = df.iloc[-1].Weekly_Sales\n",
    "    return [prognose_sales, actual_sales, actual_sales - prognose_sales]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Store 1``` mit ```Department 1```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prediction_values = get_baseline_prediction_values(multi_dict['1_1']['val_data_df'])\n",
    "print(\"Baseline Prognose:\\t{}\".format(baseline_prediction_values[0]))\n",
    "print(\"Tatsächliche Sales:\\t{}\".format(multi_dict['1_1']['prediction_goal']))\n",
    "print(\"Absoluter Fehler:\\t{}\".format(baseline_prediction_values[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So schlecht ist die Baseline-Prognose für ```1_1``` nicht. Können wir es besser?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prognose für ```Store 2``` mit ```Department 1```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prediction_values = get_baseline_prediction_values(multi_dict['2_1']['val_data_df'])\n",
    "print(\"Baseline Prognose:\\t{}\".format(baseline_prediction_values[0]))\n",
    "print(\"Tatsächliche Sales:\\t{}\".format(multi_dict['2_1']['prediction_goal']))\n",
    "print(\"Absoluter Fehler:\\t{}\".format(baseline_prediction_values[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier ist der Fehler schon bedeutend heftiger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training der neuronale Netze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden wird mit verschiedenen Typen von neuronalen Netzen trainiert.  \n",
    "Die Ergebnisse werden jeweils als neues Dictionary in das bekannte Dictionary integriert, um für die spätere Auswertung zur Verfügung zu stehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch wenn ein MLP kein RNN ist, soll es hier als Vergleichswert dienen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_dept in multi_dict.keys():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(lookback, multi_dict[store_dept]['X_train'].shape[-1])))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer=RMSprop(), loss='mae')\n",
    "    history = model.fit(multi_dict[store_dept]['X_train'],\n",
    "                        multi_dict[store_dept]['y_train'],\n",
    "                        epochs=20,\n",
    "                        validation_data=(multi_dict[store_dept]['X_test'], multi_dict[store_dept]['y_test']))\n",
    "    multi_dict[store_dept]['2MLP'] = {}\n",
    "    multi_dict[store_dept]['2MLP']['model'] = model\n",
    "    multi_dict[store_dept]['2MLP']['history'] = history\n",
    "    multi_dict[store_dept]['2MLP']['score'] = model.evaluate(multi_dict[store_dept]['X_val'], multi_dict[store_dept]['y_val'])\n",
    "    multi_dict[store_dept]['2MLP']['prediction'] = multi_dict[store_dept]['y_scaler'].inverse_transform(model.predict(multi_dict[store_dept]['X_val']).reshape(-1, 1))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_dept in multi_dict.keys():\n",
    "    model = Sequential()\n",
    "    model.add(layers.GRU(32, input_shape=(None, multi_dict[store_dept]['X_train'].shape[-1])))\n",
    "    model.add(layers.Dense(1))\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer=RMSprop(), loss='mae')\n",
    "    history = model.fit(multi_dict[store_dept]['X_train'],\n",
    "                        multi_dict[store_dept]['y_train'],\n",
    "                        epochs=20,\n",
    "                        validation_data=(multi_dict[store_dept]['X_test'], multi_dict[store_dept]['y_test']))\n",
    "    multi_dict[store_dept]['GRU'] = {}\n",
    "    multi_dict[store_dept]['GRU']['model'] = model\n",
    "    multi_dict[store_dept]['GRU']['history'] = history\n",
    "    multi_dict[store_dept]['GRU']['score'] = model.evaluate(multi_dict[store_dept]['X_val'], multi_dict[store_dept]['y_val'])\n",
    "    multi_dict[store_dept]['GRU']['prediction'] = multi_dict[store_dept]['y_scaler'].inverse_transform(model.predict(multi_dict[store_dept]['X_val']).reshape(-1, 1))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einen Dropout einzuführen sieht auf den ersten Blick sinnlos aus, wenn man bedenkt, dass uns nur 100 Datensätze zur Verfügung stehen. Inwieweit diese Befürchtungen gerechtfertigt sind, wird man in der Evaluation sehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_dept in multi_dict.keys():\n",
    "    model = Sequential()\n",
    "    model.add(layers.GRU(32, dropout=0.2, recurrent_dropout=0.2, input_shape=(None, multi_dict[store_dept]['X_train'].shape[-1])))\n",
    "    model.add(layers.Dense(1))\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer=RMSprop(), loss='mae')\n",
    "    history = model.fit(multi_dict[store_dept]['X_train'],\n",
    "                        multi_dict[store_dept]['y_train'],\n",
    "                        epochs=20,\n",
    "                        validation_data=(multi_dict[store_dept]['X_test'], multi_dict[store_dept]['y_test']))\n",
    "    multi_dict[store_dept]['GRU_drop'] = {}\n",
    "    multi_dict[store_dept]['GRU_drop']['model'] = model\n",
    "    multi_dict[store_dept]['GRU_drop']['history'] = history\n",
    "    multi_dict[store_dept]['GRU_drop']['score'] = model.evaluate(multi_dict[store_dept]['X_val'], multi_dict[store_dept]['y_val'])\n",
    "    multi_dict[store_dept]['GRU_drop']['prediction'] = multi_dict[store_dept]['y_scaler'].inverse_transform(model.predict(multi_dict[store_dept]['X_val']).reshape(-1, 1))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_dept in multi_dict.keys():\n",
    "    model = Sequential()\n",
    "    model.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, input_shape=(None, multi_dict[store_dept]['X_train'].shape[-1])))\n",
    "    model.add(layers.GRU(64, activation='relu', dropout=0.1, recurrent_dropout=0.5))\n",
    "    model.add(layers.Dense(1))\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer=RMSprop(), loss='mae')\n",
    "    history = model.fit(multi_dict[store_dept]['X_train'],\n",
    "                        multi_dict[store_dept]['y_train'],\n",
    "                        epochs=20,\n",
    "                        validation_data=(multi_dict[store_dept]['X_test'], multi_dict[store_dept]['y_test']))\n",
    "    multi_dict[store_dept]['GRU_stack'] = {}\n",
    "    multi_dict[store_dept]['GRU_stack']['model'] = model\n",
    "    multi_dict[store_dept]['GRU_stack']['history'] = history\n",
    "    multi_dict[store_dept]['GRU_stack']['score'] = model.evaluate(multi_dict[store_dept]['X_val'], multi_dict[store_dept]['y_val'])\n",
    "    multi_dict[store_dept]['GRU_stack']['prediction'] = multi_dict[store_dept]['y_scaler'].inverse_transform(model.predict(multi_dict[store_dept]['X_val']).reshape(-1, 1))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation der Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden sollen die Ergebnisse der Modelle geplottet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_results(model_results, prediction_goal):  \n",
    "    loss = model_results['history'].history['loss']\n",
    "    val_loss = model_results['history'].history['val_loss']\n",
    "    min_val_loss=np.min(val_loss)\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs[1:], loss[1:], 'bo', label='Training loss')\n",
    "    plt.plot(epochs[1:], val_loss[1:], 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print('Minimun loss on validation data:\\t{}'.format(min_val_loss))\n",
    "    print('Model score:\\t\\t\\t\\t{}'.format(model_results['score']))\n",
    "    print('Prediction Goal:\\t\\t\\t{}'.format(prediction_goal))\n",
    "    print('Prediction result:\\t\\t\\t{}'.format(model_results['prediction']))\n",
    "    print('Absolute error:\\t\\t\\t\\t{}'.format(model_results['prediction'] - prediction_goal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_results(multi_dict['1_1']['2MLP'], multi_dict['1_1']['prediction_goal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_results(multi_dict['2_1']['2MLP'], multi_dict['2_1']['prediction_goal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_results(multi_dict['1_1']['GRU'], multi_dict['1_1']['prediction_goal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_results(multi_dict['2_1']['GRU'], multi_dict['2_1']['prediction_goal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_results(multi_dict['1_1']['GRU_drop'], multi_dict['1_1']['prediction_goal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_results(multi_dict['2_1']['GRU_drop'], multi_dict['2_1']['prediction_goal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_results(multi_dict['1_1']['GRU_stack'], multi_dict['1_1']['prediction_goal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_results(multi_dict['2_1']['GRU_stack'], multi_dict['2_1']['prediction_goal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beurteilung und Schlussfolgerungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Die Modelle sehen leider bei jedem Durchlauf mit identischen Einstellungen komplett unterschiedlich aus.\n",
    "    + Wenn ein Modell einmal ein gutes Ergebnis liefert, dann nur durch Zufall.\n",
    "+ Eine Änderung der Epochen hat keinerlei sinnvolle Auswirkungen.\n",
    "+ Verschiedene Modelle lassen sich nicht miteinander vergleichen.\n",
    "+ Wir gehen davon aus, dass wir mit genau 100 Samples für Training und Test zu wenig Daten für ein sinnvolles Training zur Verfügung haben.\n",
    "\n",
    "Ein Rückschritt zur Data Preparation scheint angebracht zu sein, um eventuell mithilfe von Regression mit Bäumen bessere Ergebnisse zu erzielen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARATION: Tree Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary bauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um im Anschluss an die Modellierung mithilfe von Regressionen die Ergebnisse vergleichen zu können, benötigen wir ein neues Dictionary dafür."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Schlüssel für die erste Ebene anlegen  \n",
    "__HIER IST DIE WEICHE FÜR STORE_DEPT-Kombos__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for store_dept in set(merged_all_df.Store_Dept.tolist()):\n",
    "    tree_dict[store_dept] = {}\n",
    "print(len(tree_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit die Prüfungen im weiteren Verlauf alle funktionieren, __MUSS__ die Store-Department-Kombination ```1_1``` __IMMER__ enthalten sein!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_dept_subgroup = ['1_1']\n",
    "\n",
    "for store_dept in store_dept_subgroup:\n",
    "    tree_dict[store_dept] = {}\n",
    "print(len(tree_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation je Store und Department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/calendar_tree.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch bei den Bäumen kann die letzte Woche aus den Daten entfernt werden. Somit ist hier ebenfalls der ```{{merged_all_for_tree_df.Date.max()}}``` das jüngste Datum im Datenbestand.  \n",
    "  \n",
    "Hier benutzen wir die Woche vom ```10.12.2011``` bis zum ```16.12.2011``` zum Vorhersagen der Weihnachtswoche. Durch das Shiften der Weekly_Sales befindet sich der Wert des ```23.12.2011``` in diesem Eintrag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_dept in tree_dict.keys():\n",
    "    tmp = merged_all_for_tree_df[merged_all_for_tree_df.Store_Dept == store_dept].iloc[-2:]\n",
    "    tmp.Weekly_Sales = tmp.Weekly_Sales.shift(periods=-1)\n",
    "    tree_dict[store_dept]['val_data_df'] = tmp.iloc[0]\n",
    "    \n",
    "tree_dict['1_1']['val_data_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test-Daten je Store und Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_dept in tree_dict.keys():\n",
    "    tmp = restructure_df_for_target_variable(merged_all_for_tree_df[(merged_all_for_tree_df.Store_Dept == store_dept)])\n",
    "    tmp.Weekly_Sales = tmp.Weekly_Sales.shift(periods=-1)\n",
    "    tree_dict[store_dept]['train_test_df'] = tmp.iloc[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird der Shift umgesetzt, dazu werden jeweils zwei Zeilen des Dataframes ausgegeben und die Weekly_Sales vom unteren zum oberen versetzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dict['1_1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dict['1_1']['train_test_df'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_dept in tree_dict.keys():\n",
    "    X = tree_dict[store_dept]['train_test_df'].iloc[:,:-1]\n",
    "    y = tree_dict[store_dept]['train_test_df'].iloc[:,-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=1,shuffle=False)\n",
    "    X_scaler, y_scaler = get_X_y_scalers(tree_dict[store_dept]['train_test_df'])\n",
    "    X_train_np, foo = scale_data(X_train.drop(columns='Date').values, X_scaler)\n",
    "    y_train_np, foo = scale_data(y_train.values.reshape(-1, 1), y_scaler)\n",
    "    X_test_np, foo = scale_data(X_test.drop(columns='Date').values, X_scaler)\n",
    "    y_test_np, foo = scale_data(y_test.values.reshape(-1, 1), y_scaler)\n",
    "    foo, X_y_scaler = scale_data(tree_dict[store_dept]['train_test_df'].drop(columns='Date').values)\n",
    "    tree_dict[store_dept]['X_train'] = X_train_np\n",
    "    tree_dict[store_dept]['y_train'] = y_train_np\n",
    "    tree_dict[store_dept]['X_test'] = X_test_np\n",
    "    tree_dict[store_dept]['X_test_with_dates'] = X_test\n",
    "    tree_dict[store_dept]['y_test'] = y_test_np\n",
    "    tree_dict[store_dept]['X_scaler'] = X_scaler\n",
    "    tree_dict[store_dept]['y_scaler'] = y_scaler\n",
    "    tree_dict[store_dept]['X_y_scaler'] = X_y_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da wir bei den Bäumen nur eine Woche zum predicten benutzen kann die Funktion ```train_test_split``` verwendet werden. Die aufgesplitteten Daten skalieren wir mit unserer Funktion aus Kapitel 3.8.3. Zusätzlich zu diesen Datensets speichern wir das nicht skalierte ```X_test``` Dataset in dem Dict ab, damit die Ergebnisse später auf die Datumswerte zurückgeführt werden können (bei ```shuffle=True``` besonders wichtig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dict['1_1'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation to np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_dept in tree_dict.keys():\n",
    "    tmp = restructure_df_for_target_variable(pd.DataFrame(tree_dict[store_dept]['val_data_df']).transpose()).drop(columns='Date')\n",
    "    tmp_np, foo = scale_data(tmp.values, X_y_scaler)\n",
    "    tree_dict[store_dept]['X_val'] = tmp_np[:,:-1]\n",
    "    tree_dict[store_dept]['y_val'] = tmp_np[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dict['1_1'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set prediction goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for store_dept in tree_dict.keys():\n",
    "    tree_dict[store_dept]['prediction_goal'] = tree_dict[store_dept]['val_data_df'].Weekly_Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dict['1_1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dict['1_1']['prediction_goal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree_dict['1_1']['y_scaler'].inverse_transform(tree_dict['1_1']['y_val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELING: Tree Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im diesem Abschnitt wird GradientBoosting Regression verwendet, um ein Modell zu trainieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## .ravel() -> transforming column-vector in 1D-Array\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=400, random_state=1)\n",
    "gb_regressor.fit(tree_dict['1_1']['X_train'], tree_dict['1_1']['y_train'].ravel())\n",
    "print(gb_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dict['1_1']['y_pred'] = gb_regressor.predict(tree_dict['1_1']['X_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation der Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_mae = mean_absolute_error(tree_dict['1_1']['y_test'],tree_dict['1_1']['y_pred'])\n",
    "tree_dict['1_1']['y_scaler'].inverse_transform(np.array([gb_mae]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_feat_imp = pd.DataFrame(gb_regressor.feature_importances_).transpose()\n",
    "df_feat_imp.columns = tree_dict['1_1']['train_test_df'].columns[1:-1]\n",
    "df_feat_imp.transpose()\n",
    "df_feat_imp.transpose().sort_values(0,ascending=False).plot(kind='bar',figsize=(15,5),rot=45,legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An der Feature Importance kann abgelesen werden, dass die ```days_2_xmas``` unser wichtigstes Attribut darstellt. Durch Veränderung des Datensplits kann die Reihenfolge verändert werden (z.B. Setzt man das Attribut ```schuffle=True``` ist die Temperatur das wichtigste Attribut)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gegenüberstellung der Vorhersage und der Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates = tree_dict['1_1']['X_test_with_dates']['Date'].values\n",
    "plot_y_pred = tree_dict['1_1']['y_scaler'].inverse_transform(tree_dict['1_1']['y_pred'])\n",
    "plot_y_test = tree_dict['1_1']['y_scaler'].inverse_transform(tree_dict['1_1']['y_test'].ravel())\n",
    "\n",
    "plot_df = pd.DataFrame([test_dates,plot_y_pred,plot_y_test]).transpose().rename(columns={0:'Date',1:'pred_nw',2:'test_nw'})\n",
    "plot_df.Date = pd.to_datetime(plot_df.Date)\n",
    "plot_df.sort_values('Date').plot(x='Date',kind='bar',figsize=(15,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Achtung!:** Die Sales Werte bezeichnen den Sales für die kommende Woche, ausgehend von dem angezeigten Datum!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An dieser Grafik lässt sich ablesen, dass unsere prediction die Steigungen und Gefälle zwar berücksichtigt, aber leider der Wert nicht mit den Testdaten übereinstimmt.  \n",
    "Um die Weihnachtszeit nähert sich dies allerdings ziemlich nah an, weshalb es eventuell möglich ist unser Ziel, die Weihnachtsverkäufe 2011, mit einem guten Ergebnis vorherzusagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction with Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dict['1_1']['y_val_pred'] = gb_regressor.predict(tree_dict['1_1']['X_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gb_val_pred = tree_dict['1_1']['y_scaler'].inverse_transform(tree_dict['1_1']['y_val_pred'])\n",
    "print(gb_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beurteilung und Schlussfolgerung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "gb_val_pred[0]": "<p><strong>NameError</strong>: name &#39;gb_val_pred&#39; is not defined</p>\n",
     "tree_dict['1_1']['y_scaler'].inverse_transform(tree_dict['1_1']['y_val'])[0]": "<p><strong>NameError</strong>: name &#39;tree_dict&#39; is not defined</p>\n"
    }
   },
   "source": [
    "Die prediction für die Weihnachstzeit 2011 liefert ```{{gb_val_pred[0]}}``` als Ergebnis. Gegenüber unseres prediction goals in Höhe von ```{{tree_dict['1_1']['y_scaler'].inverse_transform(tree_dict['1_1']['y_val'])[0]}}``` ist dies leider kein gutes Ergebnis.  \n",
    "Wenn beim Splitten der Daten ```shuffle=True``` gesetzt ist, werden Werte unter 30.000$ vorhergesagt.  \n",
    "Ebenfalls getestet haben wir diese Vorhersage mit einem ```DecisionTreeRegressor``` und einem ```RandomForestRegressor```, welche jedoch beide schlechtere Ergebnisse lieferten.\n",
    "\n",
    "Da die Feature Importance die Temperatur als sehr wichtiges Attribut ausgibt, müssten die Stores weiter unterteilt werden, da das Modell eventuell nur bei Stores in Regionen mit ***normalen*** Klimabedingungen gut funktioniert, aber in unserem Datenset höchstwahrscheinlich auch welche aus Alaska, Florida, etc. enthalten sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION: Gegenüberstellung NN und Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pred = multi_dict['1_1']['2MLP']['prediction']\n",
    "gru_pred = multi_dict['1_1']['GRU']['prediction']\n",
    "gru_drop_pred = multi_dict['1_1']['GRU_drop']['prediction']\n",
    "gru_stack_pred = multi_dict['1_1']['GRU_stack']['prediction']\n",
    "y_val = tree_dict['1_1']['y_scaler'].inverse_transform(tree_dict['1_1']['y_val'])\n",
    "mpl_mae = multi_dict['1_1']['y_scaler'].inverse_transform(np.array([np.min(multi_dict['1_1']['2MLP']['history'].history['val_loss'])]))\n",
    "gru_mae = multi_dict['1_1']['y_scaler'].inverse_transform(np.array([np.min(multi_dict['1_1']['GRU']['history'].history['val_loss'])]))\n",
    "gru_drop_mae = multi_dict['1_1']['y_scaler'].inverse_transform(np.array([np.min(multi_dict['1_1']['GRU_drop']['history'].history['val_loss'])]))\n",
    "gru_stack_mae = multi_dict['1_1']['y_scaler'].inverse_transform(np.array([np.min(multi_dict['1_1']['GRU_stack']['history'].history['val_loss'])]))\n",
    "gb_tree_mae = tree_dict['1_1']['y_scaler'].inverse_transform(np.array([gb_mae]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame([[mlp_pred,gru_pred,gru_drop_pred,gru_stack_pred,gb_val_pred[0],y_val[0]]])\n",
    "eval_df = eval_df.append([[mpl_mae[0],gru_mae[0],gru_drop_mae[0],gru_stack_mae[0],gb_tree_mae[0]]]).reset_index(drop=True)\n",
    "eval_df = eval_df.rename(index={0:'prediction',1:'MAE'},columns={0:'Prediction 2 Layer Perceptron',1:'Prediction GRU',2:'Prediction GRU Dropout',3:'Prediction Stacked GRU',4:'Prediction GBR',5:'Prediction Goal'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.plot(kind='bar',rot=0, figsize=(15,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im aktuellen Durchlauf führt die Verwendung von ```GRU``` zum besten Ergebnis. Da dieses Ergebnis allerdings bei jedem Durchlauf stark schwanken, kann hier keine genaue Aussage zu getroffen werden.  \n",
    "Bei allen 5 Modellierungsvarianten liegt der ```Mean absolute error``` ziemlich hoch, da ein Unterschied von 30.000$ als erheblich anzusehen ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEPLOYMENT / Lessons Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden haben wir uns Gedanken gemacht, welche Schritte wir sinnvoll fanden, und was wir beim nächsten Mal anders machen würden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Durch den hohen Fokus auf die Phasen Data Understanding und Data Preparation haben wir gefühlt einen sehr guten Überblick über die Attribute des Datenbestandes und die inneren Zusammenhänge bekommen.\n",
    "+ Das standardisierte Vorgehen mit dem Vorhalten aller In- und Outputs über ein geschachteltes Dictionary hat uns sehr gut geholfen, den Überblick zu wahren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negativ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Die schlechten Ergebnisse bei der Modellierung der neuronalen Netze und unsere Erkenntnis, dass dies wohl an der Datenmenge und den wenigen vergangenen Weihnachten liegt, hat eine fatalistische Grundstimmung befördert.\n",
    "    + Dies könnte vor allem daran liegen, dass das erste Modell durch reinen Zufall ein exzellentes Ergebnis lieferte.\n",
    "+ Zusätzlich verursachte der Fokus auf Data Preparation, dass wir sehr wenig Zeit hatten, um uns nochmals mit frischem Kopf auf die Modellierung über Regressoren zu stürzen.\n",
    "+ Beide Umstände führten dazu, dass wir wenig besonnen manuelle Vergleiche von Ergebnissen mit verschiedenen Parametern anstellten, anstatt ein strukturiertes Vorgehen mit ```Pipelines``` und ```Grid-/Randomsearch``` zu wählen.\n",
    "\n",
    "Um diese __Probleme in der Zukunft zu vermeiden__, schlagen wir folgendes Vorgehen vor:\n",
    "+ Schnelle Exploration zu Beginn des Projekts\n",
    "    + Die \"tatsächliche\" Größe des Datensatzes wäre uns so bewusst geworden und wir hätten die kommenden Probleme erwarten können.\n",
    "    + iteratives Vorgehen statt alles brav nach Lehrbuch / CRISP-VGM\n",
    "+ Dabei bereits automatisierte Evaluation\n",
    "    + Die Nutzug von Pipelines und automatisierten Hyperparameter-Tuning-Verfahren hätte uns schnell die Wechselhaftigkeit der Ergebnisse gezeigt.\n",
    "    + Dabei sollten neben den Ergebnissen auch Visualisierungen beispielsweise der ```learning_curve()``` genutzt werden.\n",
    "+ Im Anschluss feste Meilensteine mit Deadlines für die CRISP-Phasen\n",
    "    + Dies würde helfen, begeisterte Optimierungen in den ersten Phasen zu Lasten der späteren Phasen durchzuführen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nächste Schritte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Dinge, die wir noch spannend gefunden hätten und gerne ermittelt hätten:\n",
    "    + Ist eine \"Deanonymisierung\" der einzelnen Stores (beispielsweise über ihre Wetterdaten) im Bezug auf die ca 3000 Counties in den USA möglich? Ein erstes Herangehen wäre die \"manuelle\" Berechnung der euklidischen Distanz zwischen jedem Store und jedem County. Ein zweiter Schritt wäre die Nutzung von Clustering der Stores in durch die Counties vorgegebenen Cluster.\n",
    "    + Vergleich über die Store-Department-Kombinationen hinweg un zu prüfen, ob es Kombinationen gibt, die signifikant bessere Ergebnisse liefern"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "348px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
